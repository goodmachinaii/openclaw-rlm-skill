# Architecture

## Flow diagram

```
User (Telegram)
  │
  ▼
OpenClaw Gateway (Node.js, port 18789)
  │ Model: openai-codex/gpt-5.3-codex via OAuth
  │ Already configured and running
  │
  │ [Agent detects history question]
  │ [Reads SKILL.md from rlm-engine]
  │ [Sends "Analyzing your history..." to user]
  │
  ▼
bash tool: cd ~/openclaw-rlm-skill && uv run python src/rlm_bridge.py --query "..."
  │
  ▼
rlm_bridge.py
  ├─ Loads MEMORY.md, SOUL.md, transcripts (auto-detects paths)
  ├─ Limits to 30 sessions / 2M chars (safe for 8GB RAM)
  │
  ▼
RLM (Python)
  ├─ Root LM: gpt-5.3-codex → decides analysis strategy (1 call)
  ├─ Sub-LMs: gpt-5.1-codex-mini → navigate context (2-7 calls, 4x cheaper)
  ├─ System prompt: DEFAULT from alexzhang13 (NO override)
  ├─ Local REPL: executes Python code generated by the model
  │
  ▼
CLIProxyAPI (localhost:8317)
  ├─ Compiled from source (native Go ARM64)
  ├─ Converts API calls to OAuth calls
  ├─ Uses user's ChatGPT subscription ($0 extra)
  │
  ▼
OpenAI servers → response
  │
  ▼
JSON result → OpenClaw → Telegram → User
```

## Components

### 1. OpenClaw Gateway

- Port: 18789
- Receives messages from Telegram
- Detects when to use rlm-engine skill
- Executes bridge via bash

### 2. rlm_bridge.py

Main functions:

| Function | Description |
|----------|-------------|
| `find_sessions_dir()` | Auto-detects where OpenClaw stores sessions |
| `parse_jsonl_session()` | Converts OpenClaw JSONL to readable text |
| `load_workspace()` | Loads MEMORY.md, SOUL.md, daily notes |
| `load_sessions()` | Loads up to 30 sessions (2M chars max) |
| `run_rlm()` | Executes RLM with configured models |

### 3. RLM

- Python library by alexzhang13
- Executes model-generated Python code for reasoning
- Uses local REPL (not Docker) for lower overhead
- max_depth=1 (only functional value currently)

### 4. CLIProxyAPI

- Go proxy that converts OAuth tokens to API calls
- Port: 8317
- Compiled from source (no ARM64 binaries available)
- Management interface: http://localhost:8317/management.html

## Data flow

1. User sends message on Telegram
2. OpenClaw detects deep analysis is needed
3. OpenClaw executes rlm_bridge.py with query
4. Bridge loads context (workspace + sessions)
5. Bridge invokes RLM with context
6. RLM generates and executes Python code for analysis
7. RLM makes API calls via CLIProxyAPI
8. CLIProxyAPI uses ChatGPT OAuth token
9. JSON result returns to OpenClaw
10. OpenClaw responds to user on Telegram

## Memory limits

| Resource | Limit | Reason |
|----------|-------|--------|
| Sessions | 30 | Avoid timeout |
| Characters | 2M | Safe for 8GB RAM |
| Daily notes | 200K chars | Cap for daily notes |
| Workspace files | 50K chars each | Avoid huge files |

## Model strategy

```
User query
  │
  ▼
Root LM (gpt-5.3-codex)
  │ Decides analysis strategy
  │ 1 call
  │
  ├──────────────────┐
  │                  │
  ▼                  ▼
Sub-LM 1          Sub-LM N
(gpt-5.1-codex-mini)
  │ Navigate context
  │ 2-7 calls total
  │ 4x more quota efficient
  │
  ▼
Consolidated result
```

---

# Español

## Diagrama de flujo

```
Usuario (Telegram)
  │
  ▼
OpenClaw Gateway (Node.js, puerto 18789)
  │ Modelo: openai-codex/gpt-5.3-codex via OAuth
  │ Ya configurado y funcionando
  │
  │ [Agente detecta pregunta sobre historial]
  │ [Lee SKILL.md de rlm-engine]
  │ [Envía "Analizando tu historial..." al usuario]
  │
  ▼
bash tool: cd ~/openclaw-rlm-skill && uv run python src/rlm_bridge.py --query "..."
  │
  ▼
rlm_bridge.py
  ├─ Carga MEMORY.md, SOUL.md, transcripciones (auto-detección de rutas)
  ├─ Limita a 30 sesiones / 2M chars (seguro para 8GB RAM)
  │
  ▼
RLM (Python)
  ├─ Root LM: gpt-5.3-codex → decide estrategia de análisis (1 llamada)
  ├─ Sub-LMs: gpt-5.1-codex-mini → navegan contexto (2-7 llamadas, 4x más barato)
  ├─ System prompt: DEFAULT de alexzhang13 (NO override)
  ├─ REPL local: ejecuta código Python generado por el modelo
  │
  ▼
CLIProxyAPI (localhost:8317)
  ├─ Compilado desde source (Go nativo ARM64)
  ├─ Convierte llamadas API en OAuth calls
  ├─ Usa la suscripción ChatGPT del usuario ($0 extra)
  │
  ▼
OpenAI servers → respuesta
  │
  ▼
JSON result → OpenClaw → Telegram → Usuario
```

## Componentes

### 1. OpenClaw Gateway

- Puerto: 18789
- Recibe mensajes de Telegram
- Detecta cuándo usar el skill rlm-engine
- Ejecuta el bridge via bash

### 2. rlm_bridge.py

Funciones principales:

| Función | Descripción |
|---------|-------------|
| `find_sessions_dir()` | Auto-detecta dónde OpenClaw guarda sesiones |
| `parse_jsonl_session()` | Convierte JSONL de OpenClaw a texto legible |
| `load_workspace()` | Carga MEMORY.md, SOUL.md, daily notes |
| `load_sessions()` | Carga hasta 30 sesiones (2M chars max) |
| `run_rlm()` | Ejecuta RLM con modelos configurados |

### 3. RLM

- Biblioteca Python de alexzhang13
- Ejecuta código Python generado por el modelo para razonar
- Usa REPL local (no Docker) para menor overhead
- max_depth=1 (único valor funcional actualmente)

### 4. CLIProxyAPI

- Proxy Go que convierte OAuth tokens en API calls
- Puerto: 8317
- Compilado desde source (no hay binarios ARM64)
- Interfaz de management: http://localhost:8317/management.html
