# Architecture

## Flow diagram

```
User (Telegram)
  │
  ▼
OpenClaw Gateway (Node.js, port 18789)
  │ Agent detects history question
  │ Reads SKILL.md from rlm-engine
  │ Sends "Analyzing your history..." to user
  │
  ▼
bash tool: cd ~/openclaw-rlm-skill && uv run python src/rlm_bridge.py --query "..."
  │
  ▼
rlm_bridge.py
  ├─ Reads MOONSHOT_API_KEY from environment
  ├─ Loads MEMORY.md, SOUL.md, transcripts (auto-detects paths)
  ├─ Limits to 30 sessions / 2M chars (safe for 8GB RAM)
  │
  ▼
RLM (Python)
  ├─ Root LM: kimi-k2-thinking → decides analysis strategy (1 call)
  ├─ Sub-LMs: kimi-k2.5 → navigate context (2-7 calls, 256K context)
  ├─ System prompt: DEFAULT from alexzhang13 (NO override)
  ├─ Local REPL: executes Python code generated by the model
  │
  ▼
Moonshot API (api.moonshot.ai)
  ├─ OpenAI-compatible endpoint
  ├─ Pay-as-you-go (~$0.01-0.05 per query)
  │
  ▼
Kimi models → response
  │
  ▼
JSON result → OpenClaw → Telegram → User
```

## Components

### 1. OpenClaw Gateway

- Port: 18789
- Receives messages from Telegram
- Detects when to use rlm-engine skill
- Executes bridge via bash

### 2. rlm_bridge.py

Main functions:

| Function | Description |
|----------|-------------|
| `find_sessions_dir()` | Auto-detects where OpenClaw stores sessions |
| `parse_jsonl_session()` | Converts OpenClaw JSONL to readable text |
| `load_workspace()` | Loads MEMORY.md, SOUL.md, daily notes |
| `load_sessions()` | Loads up to 30 sessions (2M chars max) |
| `run_rlm()` | Executes RLM with configured models |

### 3. RLM

- Python library by alexzhang13
- Executes model-generated Python code for reasoning
- Uses local REPL (not Docker) for lower overhead
- max_depth=1 (only functional value currently)

### 4. Moonshot API

- OpenAI-compatible API for Kimi models
- Endpoint: `https://api.moonshot.ai/v1`
- Authentication: API key via `MOONSHOT_API_KEY` environment variable
- No proxy needed — direct HTTPS calls

## Data flow

1. User sends message on Telegram
2. OpenClaw detects deep analysis is needed
3. OpenClaw executes rlm_bridge.py with query
4. Bridge loads context (workspace + sessions)
5. Bridge invokes RLM with context
6. RLM generates and executes Python code for analysis
7. RLM makes API calls to Moonshot API
8. JSON result returns to OpenClaw
9. OpenClaw responds to user on Telegram

## Memory limits

| Resource | Limit | Reason |
|----------|-------|--------|
| Sessions | 30 | Avoid timeout |
| Characters | 2M | Safe for 8GB RAM |
| Daily notes | 200K chars | Cap for daily notes |
| Workspace files | 50K chars each | Avoid huge files |

## Model strategy

```
User query
  │
  ▼
Root LM (kimi-k2-thinking)
  │ Decides analysis strategy
  │ Complex reasoning
  │ 1 call
  │
  ├──────────────────┐
  │                  │
  ▼                  ▼
Sub-LM 1          Sub-LM N
(kimi-k2.5)
  │ Navigate context
  │ 2-7 calls total
  │ 256K context window
  │
  ▼
Consolidated result
```

## Cost breakdown

| Model | Input | Output | Typical usage |
|-------|-------|--------|---------------|
| kimi-k2-thinking | ~$0.60/M | ~$2.50/M | 1 call (root) |
| kimi-k2.5 | ~$0.60/M | ~$2.50/M | 2-7 calls (sub) |
| **Per query** | | | **~$0.01-0.05** |
